{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONJUNTOS DE DATOS:\n",
    "\n",
    "test/validacion  -- test\n",
    "\n",
    "Habria que elegir el mejor modelo haciendo early stopping, haciendo promedios, etc para elegir el mejor ACC en validacion\n",
    "\n",
    "    - NUNCA SELECCIONAMOS EL MODELO USANDO TEST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING RATE:\n",
    "\n",
    "La idea es buscar un learning rate lo suficientemente grande para ir lo mas rapido posible\n",
    "\n",
    "OJO: Quizas intentar no pasarme para conseguir estabilidad (muy muy grande quizas es malo)\n",
    "\n",
    "Probar (en principio) los valores en escala logaritmica (exponenciales) 0.01 0.02 NO ---- 0.01 0.1 SI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH-SIZE:\n",
    "\n",
    "Buscar inicialización logaritmica de batch_size para ver qué nos da la mejor relación entre tiempo de ejecución y accuracy (o loss)\n",
    "\n",
    "Preestimar el tiempo de ejecución dependiendo del batch_size viendo graficas\n",
    "Lo normal es estar en torno a 50-200\n",
    "Con otro tipo de arquitecturas de red hay que tener cuidado con el batch_size porque consume memoria RAM\n",
    "\n",
    "Queremos un batch_size pequeño (para que haya muchas actualizaciones de pesos en una epoca) pero que sea lo suficientemente grande para tardar lo menos posible según el tiempo que tengas\n",
    "\n",
    "Lo interesante es ajustar un batch size y luego ajustar el resto de parámetros.\n",
    "El batch size se suele ajustar en potencias de 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCION COSTE:\n",
    "\n",
    "Utilizar diferentes funciones de LOSS implica que no podemos comparar las curvas de LOSS\n",
    "\n",
    "En algunos casos es mejor utilizar otras metricas como ACC, PREC, RECALL, F1...\n",
    "\n",
    "Para cada funcion de LOSS diferente, hay que buscar hiperparametros diferentes\n",
    "\n",
    "Normalmente para clasificacion usamos xe o hinge y para regresion usamos MSE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIONES DE ACTIVACION:\n",
    "\n",
    "relu y variantes son las que mejor resultado dan en validación en general.\n",
    "\n",
    "Redes normales (feedforward) --> relu o variantes\n",
    "\n",
    "Redes CNN --> relu o variantes\n",
    "\n",
    "Redes RNN --> tanh\n",
    "\n",
    "Sigmoid no se utiliza nunca excepto para la capa de salida si es clasificacion binaria"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OVERFITTING:\n",
    "\n",
    "En el caso en que queramos \"apagar\" neuronas usamos L1\n",
    "\n",
    "L1 puede identificar los atributos de entrada redundantes, por ejemplo, para anularlos\n",
    "\n",
    "En el caso en que queramos \"simplificar\" el modelo usamos L2\n",
    "\n",
    "Cuando se produce una diferencia muy grande entre training y validacion lo que está pasando es que el modelo se está aprendiendo \"de memoria\" los datos de entrenamiento en vez de generalizar a los de validación\n",
    "\n",
    "Tambien esta dropout para apagar neuronas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZADORES:\n",
    "\n",
    "Se recomienda utilizar Adam o Nadam. También son interesantes SGD con momento nesterov o FTRL\n",
    "\n",
    "Utilizar siempre un optimizador como Adam, Nadam u otros como RMSProp, FTRL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INICIALIZACION PESOS:\n",
    "\n",
    "La selección de la inicialización de los pesos es crucial en algunos problemas. Además, es muy sensible a las distintas activaciones que puede tener la capa. \n",
    "\n",
    "std = 1 / np.sqrt(n_neuronas_capa)\n",
    "N(0, std))\n",
    "\n",
    "La inicializacion de pesos tiene que ser RANDOM con std pequeña\n",
    "\n",
    "Probar la Normal con std optima y luego HeNormal, GlorotNormal o dejar la default = GlorotUniform\n",
    "\n",
    "Nunca poner std grande o valor continuo\n",
    "\n",
    "\n",
    "En un primer momento tiene más sentido buscar otro tipo de hiperparámetros antes que definir la inicialización de los pesos. Fijaos como elegir una activacion como RELU hace que el modelo aprenda mejor y mas rapido.\n",
    "\n",
    "Batch Normalization es una técnica util para \"forzar\" a la red a que esté en el rango de preactivaciones de aprendizaje"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4b873e07bceb12c2b9a42b36e7722015039dfeed477cfe3ebf1d2c97eb81cf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
